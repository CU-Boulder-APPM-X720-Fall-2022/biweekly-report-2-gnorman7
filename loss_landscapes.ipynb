{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss landscape exploration in MNIST Digits\n",
    "\n",
    "## Motivation\n",
    "In class we discussed a number of optimizers from a qualitative perspective. Because deep learning is a non-convex optimization, it's extremely difficult or impossible to get the same mathematical guarantees as if we were discussing convex optimization. Instead, we turn to somewhat \"intuitive\" or qualitative ideas. For instance, one of these ideas is to allow the update direction to be based on the gradient, but also based on the gradients from previous iterations, or the \"momentum\". We mentioned a [paper](https://arxiv.org/abs/1706.08500) where Adam is shown to behave as a Heavy Ball with Friction (HBF), which would allow it to skip over local minima that Stochastic Gradient Descent (SGD) would get caught in.\n",
    "\n",
    "While there are mathematical equations to derive an Ordinary Differential Equation (ODE) governing the evolution of the parameters, the resulting ODE may not have an intuitive physical meaning, or the derivation may only hold for certain optimizer parameter values. For instance, in the [derivation of Adam as a HBF](https://arxiv.org/abs/1706.08500), the optimzer parameter $\\beta_1$ and $\\beta_2$ must be chosen as specific values (for reference, these paramter control the averaging of the gradient and of the square of the gradient). Thus, while we can sometimes have a physical understanding of what an optimizer does, this is generally not the case.\n",
    "\n",
    "Therefore, we turn to other methods to examine the behavior of optimizers on a high-dimensional, non-convex loss function. One method is to visualize a loss function, and optimizers in two dimensions, via some method of dimensionality reduction of the original high-dimensional parameters. Additionally, based on lectures in class, we will choose a problem of image classification as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "PyTorch already includes many popular [datasets](https://pytorch.org/vision/stable/datasets.html) that just need to be locally downloaded. The way PyTorch includes this data makes loading easier, sometimes with the data having a designated test set that is loaded differently. Here, we'll load the training set, which we will use to make our validation set. Loading with the `transform` argument will apply the corresponding function to the inputs. We could also normalize here, but in this case it's not really a problem. Similarly, loading with the `target_transform` defined will apply that function to the outputs. For categorical data, it is good practice to use _one-hot encoding_, where the label is converted from $\\{1,2,...,n_{categories}\\}$ to $\\{ 0,1 \\}^{n_{categories}}$, where there are $n_{categories} - 1$ elements of the new vector with value `0`, and one element of the new vector with value `1`. The index of where this `1` occurs gives the original category number of the label. We'll use one-hot encoding later, not while loading the data.\n",
    "\n",
    "Throughout my code, I use some outside resources, but just to implement the ideas I'm thinking about. For instance, in this next section, I look at https://pytorch.org/tutorials/beginner/basics/data_tutorial.html, but really try to stay away from retyping the example code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: ./\n",
      "    Transforms (if any): ToTensor()\n",
      "    Target Transforms (if any): None\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000255717360D0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Label = 5')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist_data = torchvision.datasets.MNIST(root='./',download=True,train=True, transform=ToTensor())\n",
    "data_loader = torch.utils.data.DataLoader(mnist_data, batch_size=32, shuffle=False)\n",
    "print(data_loader.dataset)\n",
    "print(data_loader)\n",
    "\n",
    "# Grab a batch from the data loader\n",
    "imgs, labels = next(iter(data_loader))\n",
    "\n",
    "# Grab the first sample and label from the batch.\n",
    "img = imgs[0]\n",
    "label = labels[0]\n",
    "\n",
    "# plt.imshow(img[0,:,:]) # This was my temporary solution, and for a 1x28x28, this does the same as squeezing\n",
    "plt.figure()\n",
    "plt.imshow(img.squeeze())\n",
    "plt.title(f'Label = {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the Model\n",
    "We'll use a Convolutional Neural Network to map from the input image space $\\mathbb{R} ^{784}$ ($28 \\times 28$) to the output space $\\mathbb{R} ^{10}$ (10 categories of labels). For later convenience, call this mapping $f_\\theta : \\mathbb{R} ^{784} \\to \\mathbb{R} ^{10}$, where $\\theta$ are the parameters of the neural network. In reality, the samples have discrete pixel values between $0$ and $255$, and we will want a one-hot type output, with all zeros but a single one. However, we can still apply the function to instances that make no sense, such as an \"image\" that has pixel values larger than 255.\n",
    "\n",
    "We'll use [PyTorch's convolution function](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) `Conv2d`, which takes inputs `in_channels`, `out_channels`, `kernel_size` and through pooling outputs some metric (such as the maximum or average) over the kernel for each output channel. This [visualization](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) and this [explanation](https://towardsdatascience.com/conv2d-to-finally-understand-what-happens-in-the-forward-pass-1bbaafb0b148#:~:text=Each%20output%20channel%20is%20the,*3%3D12%20convolution%20kernels.) go through the convolution process more thoroughly than we will here.\n",
    "\n",
    "To design the CNN, I iteratively went through layers and checked the output sizes. When I got stuck with errors, I would comment out future steps / layers and check what went wrong. I learned a few things, by trial by error. First, we're usually dealing with a 4 dimensional tensor of the following form\n",
    "\n",
    "$$x \\in \\mathbb{R}^{\\text{batch size} \\times \\text{channels} \\times \\text{length} \\times \\text{height}}$$\n",
    "\n",
    "* batch size (usually 16, maybe 32... whatever I loaded in above.), the number of samples that we are loading, based on our `data_loader`\n",
    "* channels = 1, 8, 16, the number of \"color\" channels. We begin with 1 color channel (grayscale), then through the convolution process, we create parameters so that we can upsample to 8 \"color\" channels in our first `Conv2d`, and then upsample again to 16 in our second `Conv2d`.\n",
    "* length, width = 28, then varies. This was the hardest to keep track of and warrants its own discussion.\n",
    "\n",
    "### Length and Width in CNN\n",
    "For simplicity, note that length=width for the functions used here. These dimensions are the size of our image, but they change through both `Conv2D` and `AvgPool2d` (or any pooling). Because the convolutions use kernels with sizes greater than 1, when we encounter edges within our kernel, we would have to stop before we run a part of the kernel over the edge. However, by adding padding, we can just assume some value for pixels that are over the edges. With zero padding (as is default for `Conv2d`), we assume the pixel values are `0` over the edge. This means that `Conv2D` won't change the length or width. However, for pooling, both the arguments impact the length/width differently. The first argument is the `kernel_size`, which has the same problem as mentioned for `Conv2D`: we won't be able to map to the same length/width because the kernel would need information beyond the edge. The length/width are decreased by `kernel_size-1` (where we subtract `1` because a kernel of `1` returns the same image size). Then, the second input to pooling is the `stride`. This is how many pixels to skip over (in each dimension) while moving our kernel across the input image. If we choose `stride=1`, we just move to the next pixel, and this parameter does not reduce the size any. In general, this parameter divides the number of input pixels to give the output pixels (in each dimension), and we will round up. For instance, an image with 5 pixels in length (and width) and `stride=2` would give one kernel starting at the $(1,1)$, the next starting at $(1,3)$, and the last at $(1,5)$ (and then start again on $(3,1)$). Thus, we would have a resulting image with 3 pixels in each direction. Thus, the expression for the resulting length/width is `ceil(length - (kernel_size-1) / stride)`\n",
    "\n",
    "Once we are down with the convolution and pooling steps, we rescale the images into a vector, where the size of each vector of the batch is given by the length times width times number of channels. After this, we use a regular dense Multilayer Perceptron (MLP) to reduce this dimension to our expected 10 output classes.\n",
    "\n",
    "Later, we will see that the size of the model (the number of parameters) is important for what we want to do. I started with a model with $3245$ parameters, and needed to scale that down to a smaller model for runtime and memory purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.activation_fcn = nn.ReLU\n",
    "        self.convolution_part = nn.Sequential(\n",
    "            # 28 for both width and height, with 1 channel\n",
    "            nn.Conv2d(in_channels=1,out_channels=3,kernel_size=5),\n",
    "            self.activation_fcn(),\n",
    "            # 28-(5-1) = 24 for both width and height, with 8 channels\n",
    "            nn.AvgPool2d(4,2),\n",
    "            # ceil((24-4+1) / 2) = ceil(21/2) = ceil(10.5) = 11\n",
    "            nn.Conv2d(3,5,4),\n",
    "            # 11-(4-1) = 8 for width and height, 16 channels now.\n",
    "            self.activation_fcn(),\n",
    "            # Still 8\n",
    "            nn.AvgPool2d(2,2)\n",
    "            # ceil((8-(2-1))/2) = ceil(7/2) = 4\n",
    "        )\n",
    "\n",
    "        self.mlp_part = nn.Sequential(\n",
    "            # 4 by 4 reshaped into a 16 by 1 vector representing the image for one channel. Then 16 pixels by 5 channels = 80\n",
    "            nn.Linear(80, 32),\n",
    "            # Regular linear layer, output 32 units\n",
    "            self.activation_fcn(),\n",
    "            nn.Linear(32,10)\n",
    "            # Final output of 10 units. This is our representation of the 10 categories.\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the convolution part, then flatten and apply the MLP part.\n",
    "        conv_out = self.convolution_part(x)\n",
    "        mlp_out = self.mlp_part(torch.flatten(conv_out,1))\n",
    "        return mlp_out\n",
    "\n",
    "cnn_model = CNN()\n",
    "\n",
    "# Just for debugging the creation of the CNN:\n",
    "for i, data in enumerate(data_loader, 0):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data\n",
    "\n",
    "    print(inputs.shape)\n",
    "    outputs = cnn_model(inputs)\n",
    "    print(outputs.shape) \n",
    "    # print(labels.shape) \n",
    "    # print(nn.functional.one_hot(labels).shape) \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to Class\n",
    "\n",
    "In class, we discussed AlexNet, and specifically went through some of the layers. AlexNet is big, even 10 years later, so I don't load it on my computer. Instead, I try and design and understand the sizes of my own CNN. This is just like in class when we went through some of the layers of AlexNet and discussed the sizes of the convolution and pooling layers. Also, a key feature of AlexNet was using GPUs, and in this project I'm not looking at using my GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Our end goal is to look at the loss landscape and compare optimizers. Usually, we would care about an accuracy metric, but this project will already be introducing a lot of new ideas. Some of the formatting is from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html. To find a suitable learning rate, I started with a small learning rate $10^{-4}$, and very few epochs. I ran the training and showed the losses. If the loses were decaying over all the epochs and never blowing up, then I increased the learning rate. The losses blew up at a learning rate of $1.0$, so I stay below this level. Overall, this took many iterations of \"manual hyperparameter tuning\" to reach a reasonable learning rate. Then, I slowly increased the number of epochs to make sure that the training wouldn't diverge when I went for a huge run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart with a clean model\n",
    "model = CNN()\n",
    "\n",
    "epochs = 500\n",
    "lr = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "losses = []\n",
    "\n",
    "# Iterate through epochs\n",
    "for epoch in range(epochs+1):\n",
    "    loss_of_epoch = 0.0\n",
    "\n",
    "    # This will let us compute the loss on the final epoch, and not change the parameters\n",
    "    # We don't usually need this. It's just nice for what we want to do later.\n",
    "    perform_opt_step = epoch<epochs\n",
    "\n",
    "    # Iterate through batches\n",
    "    for i, data in enumerate(data_loader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        if(perform_opt_step): \n",
    "            # Zero the .grad property of all the optimizer's optimization parameters\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute the loss\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fcn(outputs,labels)\n",
    "\n",
    "            # Update the .grad property of all tensors that have requires_grad=True\n",
    "            loss.backward()\n",
    "\n",
    "            # Use the .grad property of the optimizer's optimization parameters to update them.\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # Compute the loss\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fcn(outputs,labels)\n",
    "\n",
    "        loss_of_epoch+=loss.item()\n",
    "\n",
    "    # Keep track of our training history\n",
    "    losses.append(loss_of_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runtime of the above code is quite large. For instance, running with 500 epochs (and a learning rate of $0.001$) took over 2 hours on my machine, although for a larger model while I was still figuring out this notebook. I'm not using any GPUs, although that would probably speed up the process... It would also add a lot of more complexity to what will already be a pretty substantial jump. I may undertake this as a separate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.40937618109428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epochs')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(losses[-1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "Let's save the model so if the notebook closes I don't need to rerun the training and waste more electricity. We actually just save the model parameters (with a key), and if we have an instance of the `CNN` class, we can load these parameters in the correct place (using the key). There's more information on this [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html), specifically using PyTorch's `state_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'trained_smaller_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Accuracy\n",
    "Let's check the accuracy of the model over the training set. We care about accuracy on the testing set much more, but we're going to leave that dataset alone until we've determined all the hyperparameters. I want to use the training accuracy to see if I can get away with a shorter training time (fewer epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9950333333333333"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def accuracy(model, data_loader):\n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    batch_accuracies = []\n",
    "    for i, data in enumerate(data_loader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        output_labels = torch.argmax(outputs, dim=1)\n",
    "        acc = sum(output_labels==labels)/data_loader.batch_size\n",
    "        batch_accuracies.append(acc.item())\n",
    "\n",
    "    # Because the batches are all the same size, we can just average over all the batches:\n",
    "    return mean(batch_accuracies)\n",
    "\n",
    "accuracy(model, data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I trained my model for 10 epochs and the training accuracy is already 98%? This can't be right. Let's see how an untrained model does. Good thing we write modular code so we can just reuse the accuracy function for this and other cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09751666666666667"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untrained_model = CNN()\n",
    "accuracy(untrained_model, data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check complete. The untrained model gets around 10% accuracy, which is what we expect by random chance. So, the CNN is just extremely powerful for image data, and even after 10 training epochs we're extremely accurate (at least on the training data). Let's call training complete. It's possible I'll rerun the training with more epochs when my machine isn't busy running anything else.\n",
    "\n",
    "Note that in the above, we don't use a validation set, so it's hard / impossible to tell if we are overfitting. The purpose of this notebook isn't to necessarily train a CNN well, so it's okay if we overfit and don't know about. If we did, we might see something interesting in the loss landscape later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the loss function\n",
    "\n",
    "PyTorch stores parameters in weird ways. In math, we're used to the parameters as a vector, that is $\\theta \\in \\mathbb{R}^n$, but PyTorch stores them as an iterable corresponding to the different features that each sets of the parameters are used in for the forward model. We'll convert to the vector framework as it's easier to work with and to apply mathematical theory to.\n",
    "\n",
    "In my first assignment, I looked at a least squares linear regression problem, and I also showed loss surfaces. However, for that problem, $\\theta \\in \\mathbb{R}^2$, which is drastically different than this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3245])\n"
     ]
    }
   ],
   "source": [
    "theta_opt = nn.utils.parameters_to_vector(model.parameters())\n",
    "print(theta_opt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that $\\theta \\in \\mathbb{R}^{3245}$ (and with my previous, larger model, $\\theta \\in \\mathbb{R}^{30378}$). We want to visualize the loss landscape, meaning we want to look at $L(\\theta)$ (where $L$ is the loss function). How can we visualize a function that has so many inputs?\n",
    "\n",
    "### Coordinate Transformation\n",
    "\n",
    "There are a few ways we can try. We can choose two directions, $\\theta_a , \\theta_b \\in \\mathbb{R}^{3245}$, and then look at a graph of how $L$ depends on these two directions. But now, how do we choose $\\theta_a$ and $\\theta_b$?\n",
    "\n",
    "In the simple approach, we can choose them randomly. In the difficult approach, we can choose them based off of which directions we think will be the most important. We'll start with the first, simple method, and we'll further clarify the second method when we get to it.\n",
    "\n",
    "We can measure how similar $\\theta$ is to $\\theta_a$ or $\\theta_b$ by using an inner product (which is the dot product for our case). We'll use the coordinates $(\\eta_a, \\eta_b)$ to measure how much $\\theta$ is in directions $\\theta_a$ and $\\theta_b$ (respectively). We'll also divide by a normalizing factor to make things nicer. That is,\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\eta_a & = \\frac{\\langle \\theta , \\theta_a \\rangle}{\\langle \\theta_a , \\theta_a \\rangle} \\\\\n",
    "    \\eta_b & = \\langle \\theta , \\theta_b \\rangle\n",
    "    % \\label{eq:coords}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If this is confusing, check the example of $v=[3,1,4]^T$ by using $v_a = [1, 0, 0]^T$ and $v_b = [0,1,0]^T$ (and get $\\eta_a = 3$, $\\eta_b = 1$ as the corresponding coordinates). Overall this is the same idea as [basis vectors](https://en.wikipedia.org/wiki/Basis_(linear_algebra)), but we stop at two vectors, rather than 3245 vectors, so we don't have a basis.\n",
    "\n",
    "We will also recenter the coordinate system, so that `theta_opt`, our model's final parameters is at $(0,0)$. This is just for convenience. Here's the function that will do all this math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coords(theta, theta_opt, theta_a, theta_b):\n",
    "    theta_centered = theta - theta_opt\n",
    "    eta_a = torch.dot(theta_centered,theta_a) / torch.dot(theta_a,theta_a)\n",
    "    eta_b = torch.dot(theta_centered,theta_b) / torch.dot(theta_b,theta_b)\n",
    "\n",
    "    coords = [eta_a, eta_b]\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to visualize $L(\\theta)$ for $\\theta \\in \\text{span} \\{ \\theta_a, \\theta_b \\}$. We'll choose a subset of this to actually show. We do this by choosing $(\\eta_a, \\eta_b)$ and then computing the corresponding $\\theta$ by\n",
    "\n",
    "$$\n",
    "    \\theta = \\eta_a \\theta_a + \\eta_b \\theta_b\n",
    "$$\n",
    "\n",
    "We'll make another function for this. There's a little extra math to account for the centering that already happened. This code is not general to higher dimensions of coordinates, but our purpose is to use 2 dimensions to visualize what's going on, so that is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_theta(coords, theta_opt, theta_a, theta_b):\n",
    "    theta = theta_opt + coords[0]*(theta_a) + coords[1]*(theta_b)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, verify the previous two functions. We won't stress what _kind_ of \"random\" we choose. Here's what we check. The coordinates of `theta_opt` should be $(0,0)$ and vice versa. The coordinates of $\\theta_a$ and $\\theta_b$ should be $(1,0)$ and $(0,1)$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.), tensor(0.)]\n",
      "[tensor(1.0039), tensor(0.0222)]\n",
      "[tensor(0.0092), tensor(1.0168)]\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "theta_a = torch.randn_like(theta_opt)\n",
    "theta_b = torch.randn_like(theta_opt)\n",
    "\n",
    "# We're dealing with model parameters, and we won't use backward for anything.\n",
    "with torch.no_grad():\n",
    "    coords_center = get_coords(theta_opt, theta_opt, theta_a, theta_b)\n",
    "    coords_a = get_coords(theta_a, theta_opt, theta_a, theta_b)\n",
    "    coords_b = get_coords(theta_b, theta_opt, theta_a, theta_b)\n",
    "    corresponding_theta = get_theta(coords_center, theta_opt, theta_a, theta_b)\n",
    "\n",
    "    print(coords_center)\n",
    "    print(coords_a)\n",
    "    print(coords_b)\n",
    "    print(torch.norm(corresponding_theta-theta_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Functions\n",
    "Using the random directions from before, let's visualize the loss landscape. Remember, our ultimate goal is to get a physical intuition for some of the optimization methods we discussed. This function is modified from my first project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loss_visualization(n_grid,expansion_factor,l):\n",
    "    \n",
    "    eta_a_bnds = torch.linspace(-1,1,n_grid)*expansion_factor\n",
    "    eta_b_bnds = torch.linspace(-1,1,n_grid)*expansion_factor\n",
    "\n",
    "    # # Expand the bounds so we can see edges a little more\n",
    "    # eta_a_bnds = eta_a_bnds - expansion_factor*(eta_a_bnds - torch.mean(eta_a_bnds))\n",
    "    # eta_b_bnds = eta_b_bnds - expansion_factor*(eta_b_bnds - torch.mean(eta_b_bnds))\n",
    "\n",
    "    eta_grid = torch.meshgrid(eta_a_bnds,eta_b_bnds)\n",
    "\n",
    "    loss_grid = torch.zeros(n_grid,n_grid)\n",
    "    for i in range(n_grid):\n",
    "        for j in range(n_grid):\n",
    "            eta_pt = torch.tensor([[eta_grid[0][i][j]],[eta_grid[1][i][j]]])\n",
    "            ll = l(eta_pt)\n",
    "            # print(ll)\n",
    "            loss_grid[i][j] = ll\n",
    "\n",
    "    plt.figure(figsize=[10,10])\n",
    "    plt.contourf(eta_grid[0],eta_grid[1],loss_grid,levels=100,cmap='inferno')\n",
    "    cb = plt.colorbar()\n",
    "    cb.set_label('loss')\n",
    "    # plt.scatter(Theta_history[0,0],Theta_history[0,1],s=50,marker='x',color='r',linewidth=2,label='Start')\n",
    "    # plt.plot(Theta_history[:,0],Theta_history[:,1],'-r',label='Training')\n",
    "    # plt.scatter(Theta_optimal[0],Theta_optimal[1],s=50,marker='x',color='w',linewidth=2,label='Optimal')\n",
    "    plt.legend()\n",
    "    # plt.title(r'Training of $\\Theta$')\n",
    "    plt.xlabel(r'$\\eta_a$')\n",
    "    plt.ylabel(r'$\\eta_b$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we need a function that takes $(\\eta_a, \\eta_b)$ and returns the loss associated with that point. This is what is used as `l(eta_pt)` in the above function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model,data_loader):\n",
    "    # For a model and a data loader, returns the loss for the current parameters\n",
    "\n",
    "    # This is the same code as in the training loop, without optimization stuff.\n",
    "    # I'm using loss_fcn as a global variable here, which is not ideal, but it's not critical.\n",
    "    loss_of_epoch = 0.0\n",
    "\n",
    "    # Iterate through batches\n",
    "    for i, data in enumerate(data_loader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Compute the loss\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fcn(outputs,labels)\n",
    "        loss_of_epoch+=loss\n",
    "\n",
    "    return loss_of_epoch\n",
    "\n",
    "def theta_to_loss(theta):\n",
    "    # Saves theta as parameters for a new model, then evaluates the loss\n",
    "    temp_model = CNN()\n",
    "    # This will put the theta parameters directly into the model.\n",
    "    nn.utils.vector_to_parameters(theta, temp_model.parameters())\n",
    "    return get_loss(temp_model,data_loader)\n",
    "\n",
    "def coords_to_loss(eta_pt, theta_opt, theta_a, theta_b):\n",
    "    theta = get_theta(eta_pt, theta_opt, theta_a, theta_b)\n",
    "    return theta_to_loss(theta)\n",
    "\n",
    "l = lambda cs : coords_to_loss(cs, theta_opt, theta_a, theta_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code block was all to create our loss, as a function of our transformed coordinates $(\\eta_a, \\eta_b)$. Just one last check. We should get the loss that we ended training with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59.67607614146155, 46.07084206480412, 51.531927822153875, 48.338220096627886, 34.40937618109428]\n"
     ]
    }
   ],
   "source": [
    "print(losses[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(34.4093, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(34.4093, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(theta_to_loss(theta_opt))\n",
    "l([0.0,0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_loss_visualization(5,0.01,l)\n",
    "plt.plot(0,0,'wx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above we see a pretty gradual change between loss values. We're also looking at a relatively small area (a small change in parameters), but within that area, the change is very small too. There are no special or unusual features that we see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A different way to choose $\\theta_a$ and $\\theta_b$\n",
    "\n",
    "In the above, we choose the directions $\\theta_a$ and $\\theta_b$ randomly. This works to reduce the dimensionality of what we look at, but it doesn't offer us a helpful insight into the loss surface.\n",
    "\n",
    "A better idea is to look at the important eigenvectors of the Hessian of the loss function. The [Hessian](https://en.wikipedia.org/wiki/Hessian_matrix) is defined as the following:\n",
    "\n",
    "$$ \\mathbf{H}( \\theta ) = \\nabla ^2 _\\theta L (\\theta) $$\n",
    "\n",
    "or in other words,\n",
    "\n",
    "$$(\\mathbf{H})_{i,j} = \\frac{\\partial^2 L}{\\partial x_i \\partial x_j}$$\n",
    "\n",
    "That is, the Hessian describes the way that the gradient of $L(\\theta)$ changes with respect to the parameters $\\theta$. As mentioned in class, we use the gradient to update our parameters, but the Hessian is too expensive to compute and too use. Recall that in Newton's Method, we would invert the Hessian and use this for each optimization step.\n",
    "\n",
    "Originally, I had a model with $30378$ parameters. This would have meant that $\\mathbf{H} \\in \\mathbb {R} ^{30378 \\times 30378}$. Storing such a matrix would have $30378^2 = 922822884$ entries, with each entry having 4 bytes of memory for a `float32`. This would require about $3.7$ gigabytes of memory. Then, this matrix would be used to perform a few operations (an eigenvalue decomposition). For my computer, that is a bit too much, so I retrained a smaller model with a Hessian that takes $0.042$ gigabytes to store.\n",
    "\n",
    "As we mentioned in class, computing the Hessian requires additional gradient computations equal to the number of parameters. For us, we have $3245$ parameters, so for a 10 second computation, that is roughly a 9 hour computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[5, 5]}, size=[1]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-b5c6acd41d4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp_grad\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdloss_dtheta\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mp_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mjac\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[5, 5]}, size=[1]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)"
     ]
    }
   ],
   "source": [
    "# loss_fcn = nn.CrossEntropyLoss()\n",
    "\n",
    "# temp_model = CNN()\n",
    "# # This will put the theta parameters directly into the model.\n",
    "# nn.utils.vector_to_parameters(theta_opt, temp_model.parameters())\n",
    "# # l = get_loss(temp_model,data_loader)\n",
    "\n",
    "# # From a pytorch forum\n",
    "# n =  sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# def f(theta_opt):\n",
    "#     nn.utils.vector_to_parameters(theta_opt, temp_model.parameters())\n",
    "#     l = get_loss(temp_model,data_loader)\n",
    "#     return l\n",
    "\n",
    "# dloss_dtheta = []\n",
    "\n",
    "# f_eval = f(theta_opt)\n",
    "# f_eval.backward()\n",
    "\n",
    "# for param in temp_model.parameters():\n",
    "#     dloss_dtheta.append(param.grad)\n",
    "\n",
    "# jac = torch.zeros((n,1))\n",
    "# i = 0\n",
    "# for p_grad in dloss_dtheta:\n",
    "#     for p in p_grad:\n",
    "#         jac[i] = p\n",
    "#         i+=1\n",
    "\n",
    "# H = torch.autograd.functional.jacobian(theta_to_loss, theta_opt, create_graph=True)\n",
    "\n",
    "# print(H.shape)\n",
    "# print(H==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(H==0)\n",
    "# print(H)\n",
    "# print(theta_opt.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-42-8cb4d438ac88>:8: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  g = nn.utils.parameters_to_vector(temp_model.parameters()).grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-42-8cb4d438ac88>:11: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  g = nn.utils.parameters_to_vector(temp_model.parameters()).grad\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None,)\n"
     ]
    }
   ],
   "source": [
    "# env_loss = theta_to_loss(theta_opt)\n",
    "# env_grads = torch.autograd.grad(env_loss, theta_opt, retain_graph=True, create_graph=True, allow_unused=True)\n",
    "\n",
    "print( env_grads[0] )\n",
    "# hess_params = torch.zeros_like(env_grads[0])\n",
    "# for i in range(env_grads[0].size(0)):\n",
    "#     for j in range(env_grads[0].size(1)):\n",
    "#         hess_params[i, j] = torch.autograd.grad(env_grads[0][i][j], theta_opt, retain_graph=True)[0][i, j] #  <--- error here\n",
    "# print( hess_params )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('spyder-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14a0abb8b133e72568b7a6e8fc97e5007cb14527d4a9bd19b762b2aa54dc2329"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
